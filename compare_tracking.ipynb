{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: compare tracking \n",
    "\n",
    "This case study is compare the different case studies abilities to track the user based on a standard video. \n",
    "By drawing the boundary boxes or landmarks on the video. Then the videos will be saved so that users can manually evaluate the tracking based on the videos.\n",
    "\n",
    "The face detection models compared will be: \n",
    "- Haar Cascade\n",
    "- HOG\n",
    "- DNN\n",
    "- MMOD\n",
    "\n",
    "These detection models will also be compared against the following library: \n",
    "- CVZone\n",
    "\n",
    "(**Note:** We also test MMOD here to evaluate if it has potential to be used, even though it is too cost expensive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dependencies \n",
    "\n",
    "Run the next codeblock to import the `models` module with all the code, and import any other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages needed \n",
    "import cv2\n",
    "import dlib\n",
    "from cvzone.FaceMeshModule import FaceMeshDetector\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Import package for changing the path\n",
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Importing the functions for the face detection models \n",
    "from models.code.dnn import detect_face_dnn\n",
    "from models.code.haar import detect_face_haar\n",
    "from models.code.hog import detect_face_hog\n",
    "from models.code.cvzone import detect_face_cvzone\n",
    "from models.code.mmod import detect_face_mmod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The testing video \n",
    "\n",
    "For creating the tracking videos, we can use any trivial video of a user moving. In this case study, we will be using a video from the Intel IoT Development Kit (see [resources](#resources)). The video is about 2 minutes long, and involves a man moving his head around. \n",
    "\n",
    "Run the codeblock below to set path variables and the color of the boundary boxes or landmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the video exist \n",
    "path_to_video = \"./datasets/tracking/tracking_video.mp4\"\n",
    "output_dir = \"./results/tracking_videos/\"\n",
    "\n",
    "# Also set the desired color to be used\n",
    "COLOR = (0, 255, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tracking the face \n",
    "\n",
    "Each face detection method has detectors. They are also used in the [OpenCV Server](https://github.com/RIT-NTNU-Bachelor/OpenCV_Server/blob/main/tests/test_utils.py). Run this codeblock: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1714069835.519054   28660 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1714069835.521084   34302 gl_context.cc:357] GL version: 3.2 (OpenGL ES 3.2 Mesa 23.2.1-1ubuntu3.1~22.04.2), renderer: RENOIR (renoir, LLVM 15.0.7, DRM 3.54, 6.5.0-28-generic)\n"
     ]
    }
   ],
   "source": [
    "# Load models \n",
    "HAAR_CLASSIFIER = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "HOG_DETECTOR = dlib.get_frontal_face_detector()\n",
    "MMOD_DETECTOR = dlib.cnn_face_detection_model_v1(\"./models/trained_models/mmod_human_face_detector.dat\")\n",
    "CVZONE_DETECTOR = FaceMeshDetector()\n",
    "DNN_CAFFE_MODEL_PATH = \"./models/trained_models/res10_300x300_ssd_iter_140000_fp16.caffemodel\"\n",
    "DNN_CONFIG_PATH = \"./models/trained_models/deploy.prototxt\"\n",
    "DNN_NET = cv2.dnn.readNetFromCaffe(DNN_CONFIG_PATH, DNN_CAFFE_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will help draw the result to the video. They are also used in the [Unit Test for the OpenCV Server](https://github.com/RIT-NTNU-Bachelor/OpenCV_Server/blob/main/tests/test_utils.py):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_rectangle(face_coords, image):\n",
    "    \"\"\"Function to draw a single rectangle\n",
    "\n",
    "    Args:\n",
    "    - face_coords (tuple): Tuple with x, y, width and height of the bounding box\n",
    "    - image (Matlike): Image object obtained from the OpenCV imread function \n",
    "    \"\"\"\n",
    "    x, y, width, height = face_coords\n",
    "    cv2.rectangle(image, (x, y), (x + width, y + height), COLOR, 2)\n",
    "\n",
    "\n",
    "def draw_landmark(landmarks, image):\n",
    "    \"\"\"Function to draw a landmark.\n",
    "    Draws a circle in the image for each landmark in the given list of landmarks\n",
    "\n",
    "    Args:\n",
    "        landmarks (list): List of landmarks where each landmark has a x and y position\n",
    "        image (Matlike): Image object obtained from the OpenCV imread function \n",
    "    \"\"\"\n",
    "    for point in landmarks:\n",
    "        cv2.circle(image, (point[0], point[1]), 1, COLOR, -1)\n",
    "\n",
    "def draw_rectangle_from_dlib(face_rectangle, img_with_box):\n",
    "    \"\"\"Function that draws the given dlib rectangle to the given image\n",
    "\n",
    "    Args:\n",
    "        face_rectangle (dlib.rectangle): A dlib rectangle object\n",
    "        img_with_box (Matlike): Image object obtained from the OpenCV imread function\n",
    "    \"\"\"\n",
    "    cv2.rectangle(img_with_box, (face_rectangle.left(), face_rectangle.top()), (face_rectangle.right(), face_rectangle.bottom()), COLOR, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function will help process a frame based on method. It iterates over each method and then uses some logic to decide what type of drawing function should be utilized. It takes a list of detectors and writers.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_write_frame(frame, detectors, writers):\n",
    "    \"\"\"Process frame for each detection method and write to appropriate outputs using specific drawing logic for each.\"\"\"\n",
    "    for method, detector in detectors.items():\n",
    "        face = detector(frame)\n",
    "        frame_copy = frame.copy()\n",
    "        \n",
    "        if face is not None:\n",
    "            # Check what model it is based on the datatype and then draw the landmark/rectangle accordantly \n",
    "            if isinstance(face, tuple) or (isinstance(face, np.ndarray) and face.ndim == 1): # Single face as a tuple or 1D numpy array \n",
    "                draw_rectangle(face, frame_copy)\n",
    "            elif isinstance(face, np.ndarray) and face.ndim == 2: # Multiple faces in a 2D numpy array\n",
    "                for single_face in face:\n",
    "                    draw_rectangle(single_face, frame_copy)\n",
    "            elif isinstance(face, dlib.rectangle):  # Single dlib rectangle => 1 face\n",
    "                draw_rectangle_from_dlib(face, frame_copy)\n",
    "            elif isinstance(face, dlib.rectangles):  # Multiple dlib rectangles => 2 faces\n",
    "                for face_rectangle in face:\n",
    "                    draw_rectangle_from_dlib(face_rectangle, frame_copy)\n",
    "            elif isinstance(face, list): # DNN or CVZone\n",
    "                if all(isinstance(f, tuple) and len(f) == 4 for f in face):  # List of tuples for DNN => 2 faces\n",
    "                    for f in face:\n",
    "                        draw_rectangle(f, frame_copy)\n",
    "                elif len(face) == 468:  # CVZone => 1 face\n",
    "                    draw_landmark(face,frame_copy)\n",
    "                elif all(isinstance(f, list) for f in face) and all(len(f) == 468 for f in face):  #CVZone => 2 faces\n",
    "                    for face_landmarks in face:\n",
    "                        draw_landmark(face_landmarks, frame_copy)\n",
    "\n",
    "        writers[method].write(frame_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next codeblock processes the video and creates new videos for each method: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main processing setup\n",
    "cap = cv2.VideoCapture(path_to_video)\n",
    "width, height = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)), int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Prepare the output video writers for each type and detection functions\n",
    "methods = [\"haar\", \"dnn\", \"hog\", \"cvzone\" , \"mmod\"]\n",
    "writers = {method: cv2.VideoWriter(f\"{output_dir}{method}.mp4\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (width, height)) for method in methods}\n",
    "\n",
    "# All the detector \n",
    "detectors = {\n",
    "    \"haar\": lambda img: detect_face_haar(img, HAAR_CLASSIFIER, detect_multiple_faces=False),\n",
    "    \"dnn\": lambda img: detect_face_dnn(img, DNN_NET, detect_multiple_faces=False),\n",
    "    \"hog\": lambda img: detect_face_hog(img, HOG_DETECTOR, detect_multiple_faces=False),\n",
    "    \"cvzone\": lambda img: detect_face_cvzone(img, CVZONE_DETECTOR, detect_multiple_faces=False),\n",
    "    \"mmod\": lambda img: detect_face_mmod(img, MMOD_DETECTOR, detect_multiple_faces=False)\n",
    "}\n",
    "\n",
    "# Processing loop\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Process frame and write to multiple outputs\n",
    "    process_and_write_frame(frame, detectors, writers)\n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "for writer in writers.values():\n",
    "    writer.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that the videos has been created. There should be a total of 5: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Video file found: ./results/tracking_videos/hog.mp4\n",
      "[INFO] Video file found: ./results/tracking_videos/haar.mp4\n",
      "[INFO] Video file found: ./results/tracking_videos/dnn.mp4\n",
      "[INFO] Video file found: ./results/tracking_videos/cvzone.mp4\n",
      "[INFO] Video file found: ./results/tracking_videos/mmod.mp4\n",
      "Total video files created: 5\n"
     ]
    }
   ],
   "source": [
    "output_count = 0\n",
    "\n",
    "for filename in glob.iglob(f'{output_dir}/**', recursive=True):\n",
    "    if os.path.isfile(filename) and filename.endswith(\".mp4\"):\n",
    "        print(f\"[INFO] Video file found: {filename}\")\n",
    "        output_count += 1\n",
    "\n",
    "\n",
    "print(f\"Total video files created: {output_count}\")\n",
    "assert(output_count == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all videos has been created, one can manually review them to check the algorithms ability to track the movement of the users head."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "Intel IoT Developer Kit Sample Videos: <br>\n",
    "https://github.com/intel-iot-devkit/sample-videos?tab=readme-ov-file#samples-videos \n",
    "\n",
    "Check out the GitHub repository here: [GitHub Repository](https://github.com/RIT-NTNU-Bachelor/case-study)\n",
    "\n",
    "**Created by:** Kjetil Indrehus, Sander Hauge and Martin Johannessen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
